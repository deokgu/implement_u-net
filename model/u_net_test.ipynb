{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "class u_net(nn.Module):\n",
    "    def double_conv(self, in_channels, out_channels):\n",
    "        \"\"\"\n",
    "            It consists of the repeated application of two 3x3 convolutions (unpadded convolutions),\n",
    "            each followed by a rectified linear unit (ReLU) \n",
    "        \"\"\"\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def __init__(self,):\n",
    "        super().__init__()\n",
    "\n",
    "        self.down_dconv_1 = self.double_conv(3, 64) \n",
    "        self.maxpool_2x2_1 = nn.MaxPool2d(kernel_size= 2, stride=2) # 2x2 max pooling operation with stride 2 for downsampling. At each downsampling step we double the number of feature channels\n",
    "        \n",
    "        self.down_dconv_2 = self.double_conv(64, 128)\n",
    "        self.maxpool_2x2_2 = nn.MaxPool2d(kernel_size =2, stride=2)\n",
    "\n",
    "        self.down_dconv_3 = self.double_conv(128, 256)\n",
    "        self.maxpool_2x2_3 = nn.MaxPool2d(kernel_size =2, stride=2)\n",
    "\n",
    "        self.down_dconv_4= self.double_conv(256, 512)\n",
    "        self.maxpool_2x2_4 = nn.MaxPool2d(kernel_size =2, stride=2)\n",
    "\n",
    "        self.down_dconv_5 = self.double_conv(512, 1024)\n",
    "\n",
    "        self.up_trans_4 = nn.ConvTranspose2d(in_channels=1024, out_channels=512, kernel_size=2, stride=2) # Every step in the expansive path consists of an upsampling of the feature map followed by a 2x2 convolution\n",
    "        self.up_conv_4 = self.double_conv(1024,512)\n",
    "        \n",
    "        self.up_trans_3 = nn.ConvTranspose2d(in_channels=512, out_channels=256, kernel_size=2, stride=2)\n",
    "        self.up_conv_3 = self.double_conv(512,256)\n",
    "        \n",
    "        self.up_trans_2 = nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=2, stride=2)\n",
    "        self.up_conv_2 = self.double_conv(256,128)\n",
    "\n",
    "        self.up_trans_1 = nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=2, stride=2)\n",
    "        self.up_conv_1 = self.double_conv(128,64)\n",
    "\n",
    "        self.out = nn.Conv2d(in_channels=64, out_channels=2, kernel_size=1) # 1x1 convolution\n",
    "\n",
    "    def forward(self, x): \n",
    "        down_dconv_1 = self.down_dconv_1(x) # check down_dconv_1\n",
    "        maxpool_1 = self.maxpool_2x2_1(down_dconv_1) \n",
    "\n",
    "        down_dconv_2 = self.down_dconv_2(maxpool_1) # check  down_dconv_2\n",
    "        maxpool_2 = self.maxpool_2x2_2(down_dconv_2)\n",
    "\n",
    "        down_dconv_3 = self.down_dconv_3(maxpool_2) # check  down_dconv_3\n",
    "        maxpool_3 = self.maxpool_2x2_3(down_dconv_3)\n",
    "\n",
    "        down_dconv_4 = self.down_dconv_4(maxpool_3)  #check  down_dconv_4\n",
    "        maxpool_4 = self.maxpool_2x2_4(down_dconv_4)\n",
    "\n",
    "        down_dconv_5 = self.down_dconv_5(maxpool_4) \n",
    "        # end constracting path \n",
    "\n",
    "        # start expansive path \n",
    "        up_trans_4 = self.up_trans_4(down_dconv_5)\n",
    "        cat_4 = torch.cat([down_dconv_4, up_trans_4],dim=1)  # dim =[0:batch, 1: channel, 2: height, 3: width]\n",
    "        up_conv_4 = self.up_conv_4(cat_4)\n",
    "\n",
    "        up_trans_3 = self.up_trans_3(up_conv_4)\n",
    "        cat_3 = torch.cat([down_dconv_3, up_trans_3],dim=1)  \n",
    "        up_conv_3 = self.up_conv_3(cat_3)\n",
    "\n",
    "        up_trans_2 = self.up_trans_2(up_conv_3)\n",
    "        cat_2 = torch.cat([down_dconv_2, up_trans_2],dim=1)  \n",
    "        up_conv_2 = self.up_conv_2(cat_2)\n",
    "\n",
    "        up_trans_1 = self.up_trans_1(up_conv_2)\n",
    "        cat_1 = torch.cat([down_dconv_1, up_trans_1],dim=1)  \n",
    "        up_conv_1 = self.up_conv_1(cat_1)\n",
    "\n",
    "        out = self.out(up_conv_1)\n",
    "        return out \n",
    "u_net()"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-08e2ff717403>, line 48)",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-08e2ff717403>\"\u001b[0;36m, line \u001b[0;32m48\u001b[0m\n\u001b[0;31m    def forward(self,)L:\u001b[0m\n\u001b[0m                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "98b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}